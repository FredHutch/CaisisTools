{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CaisisImport. A utility to prepare a Caisis-to-Excel export for import into Oncoscape (via cBioPortal format).\n",
    "\n",
    "dataset_folder = '../Caisis_NonPublicData/Brain_nov2021'\n",
    "diseaseChosen = 'Brain'  \n",
    "study_identifier = 'brain_abc'  #used for copying brain_abc_custom_caisis_prep.py to this folder as custom_caisis_prep.py.\n",
    "type_of_cancer = \"misc\"  # Use cbioportal term\n",
    "\n",
    "# dataset_folder = '../Caisis_NonPublicData/Sarcoma_nov2021'   \n",
    "# diseaseChosen = 'Sarcoma'  #'Brain'\n",
    "# study_identifier = 'sarcoma_caisis'  \n",
    "# type_of_cancer = \"misc\"  # Use cbioportal term\n",
    "\n",
    "# dataset_folder = '../Caisis_NonPublicData/Liver_nov2021'   \n",
    "# diseaseChosen = 'Liver Cancer'   \n",
    "# study_identifier = 'liver_caisis' \n",
    "# type_of_cancer = \"misc\"  # Use cbioportal term\n",
    "\n",
    "# dataset_folder = '../Caisis_NonPublicData/Pancreas_nov2021'   \n",
    "# diseaseChosen = 'Pancreas Cancer'  #'Brain'\n",
    "# study_identifier = 'pancreas_caisis'  \n",
    "# type_of_cancer = \"misc\"  # Use cbioportal term\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Steps:\n",
    "# . Limit by disease\n",
    "# . Read in CSVs\n",
    "# . Export as TSVs\n",
    "# . ZeroDates\n",
    "# . \n",
    "\n",
    "# oncoscape_bar_override: {\"version\": \"1.0\", \"style\": \"Symbols\", \"shape\": \"circle\", \"subtypeColors\": {\"rp\": \"#FF0000\", \"xrt\": \"#00FF00\"}}\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import errno\n",
    "import sys\n",
    "import math\n",
    "import shutil\n",
    "import pipes\n",
    "import re\n",
    "from typing import List\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "from os.path import exists \n",
    "\n",
    "from zero_dates import   zero_dates\n",
    "\n",
    "global custom_prep_filename\n",
    "\n",
    "#===============================================================\n",
    "custom_prep_fullpath = os.path.join(\".\", study_identifier +\"_custom_caisis_prep.py\") # Can override this to any location you like.\n",
    "study_description = study_identifier + ' study'\n",
    "\n",
    "## knownTables = ['Demographics', 'Encounters', 'ClinicalStage', 'LabTests', 'PathTest', 'SocialHistory', 'LabTestGenetics', 'PathStageGrade', 'RadiationTherapy',  ]\n",
    "# table Status is treated separately, first.\n",
    "knownTables = ['Clinical Stages', 'Demographics', 'Encounters', 'Medical Therapy', 'Pathology', 'Procedures', 'Radiation', 'Social History', 'Status']\n",
    "date_format = '%Y-%m-%d'  # e.g. 2021-01-23\n",
    "\n",
    "#days_before_dx_to_include = 0  # if 30, can include events up to 30 days before the zero date. Default is 30.\n",
    "global missing_date_str\n",
    "missing_date_str = '2222-02-02'  # use instead of empty/None to indicate missing data\n",
    "\n",
    "missing_date = datetime.strptime(missing_date_str, date_format)  #datetime(2222, 2,2)  # TBD: \n",
    "\n",
    "\n",
    "## -- internal initialization --\n",
    "patients_first_dx = pd.DataFrame()   # just PatientId and DiagnosisDate\n",
    "data_clinical_patient = pd.DataFrame() # Will become the data_clinical_patient table.\n",
    "loaded_tables_dict = {}\n",
    "data_clinical_patient = None\n",
    "data_clinical_timeline_dfs = {}   # dictionary of dataframes, keyed off of \"timeline-foo\" names.\n",
    "data_clinical_timeline_graph_markers = {}  # dictionary of graph_marker_type values, to define arcs, bars, squares, diamonds, circles, triangles.\n",
    "datafiles_fields = {}\n",
    "current_lookup_field = None  # placeholder.\n",
    "noval_list = []   # List of patient IDs with no associated value in one or more of the import_fields. Use this for reporting. \n",
    "\n",
    "foldername_with_headers = '01_with_headers'\n",
    "foldername_zero_dates = '02_zero_dates'\n",
    "custom_prep_filename = \"custom_caisis_prep.py\"\n",
    "has_custom_prep_file = False\n",
    "# src_path = dataset_folder\n",
    "# dst = \".\"\n",
    "\n",
    "file_path = custom_prep_fullpath \n",
    "if  exists(file_path):\n",
    "    print(\"== custom prep exists.\")\n",
    "    shutil.copy(file_path, os.path.join(\".\", custom_prep_filename))\n",
    "    has_custom_prep_file = True\n",
    "    if 'custom_caisis_prep' in sys.modules.keys():\n",
    "        print(\"====================prepmod exists   \")\n",
    "        del sys.modules['custom_caisis_prep']\n",
    "    import custom_caisis_prep\n",
    "    print(\"testing...\")\n",
    "    print(custom_caisis_prep.version)\n",
    "else:\n",
    "    print(\"== no custom prep exists.\")\n",
    "    if  exists(\"./\"+custom_prep_filename):\n",
    "        os.remove(\"./\"+custom_prep_filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_marker_types = {\n",
    "    \"arcs\"    : { \"mtype\": \"Arcs\", \"shape\":None},\n",
    "    \"bars\"    : { \"mtype\": \"Bars\", \"shape\":None},\n",
    "    \"circles\" : { \"mtype\": \"Symbols\", \"shape\":\"circle\"},\n",
    "    \"squares\" : { \"mtype\": \"Symbols\", \"shape\":\"square\"},\n",
    "    \"triangles\": { \"mtype\": \"Symbols\", \"shape\":\"triangle\"},\n",
    "    \"diamonds\" : { \"mtype\": \"Symbols\", \"shape\":\"diamond\"},\n",
    "    \"stars\"    : { \"mtype\": \"Symbols\", \"shape\":\"star\"},  # Not yet implemented\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImportField:\n",
    "    source_name:str\n",
    "    final_name:str\n",
    "    type:str = 'STRING'  # STRING, NUMBER, or DATE (DATE gets turned into STING in final TSV files.)\n",
    "    conversion_function = None\n",
    "\n",
    "    def __init__(self, source_name, final_name, type=\"STRING\", conversion_function=None ):\n",
    "        self.source_name = source_name\n",
    "        self.final_name = final_name\n",
    "        self.type = type\n",
    "        self.conversion_function = conversion_function\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.source_name +\"->\"+ self.final_name+\", type=\"+self.type+\".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_timeline_fields = [\n",
    "    ImportField('EVENT_TYPE', final_name='EVENT_TYPE') , \n",
    "    ImportField('START_DATE', final_name='START_DATE', type='DATE') , \n",
    "    ImportField('STOP_DATE', final_name='STOP_DATE', type='DATE') , \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_file_and_folder(filename, obj):\n",
    "    if not os.path.exists(os.path.dirname(filename)):\n",
    "        try:\n",
    "            os.makedirs(os.path.dirname(filename))\n",
    "        except OSError as exc: # Guard against race condition\n",
    "            if exc.errno != errno.EEXIST:\n",
    "                raise\n",
    "\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(obj)\n",
    "\n",
    "def patients_and_descriptive_header(df:pd.DataFrame, header):\n",
    "    # TBD: note header in separate place\n",
    "    str_ids = df.to_string(index=False)\n",
    "    a =  str_ids  # header + \"\\n\" + str_ids\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_patients_first_dx():\n",
    "    global patients_first_dx, data_clinical_patient, dataset_folder\n",
    "    print(\"\\nSTEP 1: Find patients' first diagnosis date.\")\n",
    "    print(dataset_folder)\n",
    "    status_fullpath = dataset_folder +'/raw_csv/Status.csv'\n",
    "    if not os.path.exists(dataset_folder +'/raw_csv'):\n",
    "        sys.exit('ERROR: Cannot find folder \"raw_csv\".')\n",
    "\n",
    "    if not os.path.exists(status_fullpath):\n",
    "        sys.exit('ERROR: Status.csv file not found at ' + status_fullpath)\n",
    "\n",
    "    tbl_status = pd.read_csv(status_fullpath) #'Demographics.csv')\n",
    "    tbl_status['Date'] =pd.to_datetime(tbl_status.StatusDate)\n",
    "    tbl_status['PatientId'] =  tbl_status['PatientId'].astype(str)\n",
    "\n",
    "    dxRows = tbl_status[tbl_status.Status.eq('Diagnosis Date')]\n",
    "    diseaseRows = dxRows[dxRows.StatusDisease.eq(diseaseChosen)].sort_values('Date')\n",
    "\n",
    "    dxNanRows = diseaseRows[diseaseRows.Date.isna()][['PatientId']]\n",
    "    if(dxNanRows.size > 0):\n",
    "        print('CHECK NoDxDate? REPORT NoDxDate.txt has *' +str(dxNanRows.size)+'* diagnoses of '+diseaseChosen+' without diagnosis dates.')\n",
    "        report_body = patients_and_descriptive_header(dxNanRows, 'Patients with \"'+diseaseChosen+\" but no DiagnosisDate:\")\n",
    "    #    save_file_and_folder('reports/NoDxDate.txt', report_body)\n",
    "        save_file_and_folder(dataset_folder+'/reports/NoDxDate.txt', report_body)\n",
    "    else:\n",
    "        print('CHECK NoDxDate? OK')\n",
    "        print('TBD: delete existing NoDxDate.txt report.')\n",
    "        if os.path.exists(dataset_folder+'/reports/NoDxDate.txt'):\n",
    "            os.remove(dataset_folder+'/reports/NoDxDate.txt')\n",
    "            print('REMOVED NoDxDate.txt')\n",
    "            \n",
    "    diseaseDatedRows = diseaseRows[diseaseRows.Date.isna()==False]\n",
    "    patientid_date_dict = {}\n",
    "    print(\"===START PATIENT LOOP===\")\n",
    "    for index, row in diseaseDatedRows.iterrows():\n",
    "        pid = str(row['PatientId'])\n",
    "        if((pid in patientid_date_dict) == False):\n",
    "            justYMD = row['Date']  #datetime.strftime(row['Date'], \"%Y-%m-%d\")\n",
    "            patientid_date_dict[pid] = justYMD\n",
    "            \n",
    "        else:\n",
    "            pass\n",
    "    print(\"===END PATIENT LOOP===\")\n",
    "\n",
    "    print('Resulting patient IDs = ' + str(len(patientid_date_dict)))   \n",
    "\n",
    "    data = []\n",
    "    for key in patientid_date_dict.keys():\n",
    "        new_row = [key, patientid_date_dict[key]]\n",
    "        data.append(new_row)\n",
    "    patients_first_dx = pd.DataFrame(data, columns=[\"PatientId\", \"DiagnosisDate\"])\n",
    "    data_clinical_patient = patients_first_dx.copy()\n",
    "\n",
    "    report_body = patients_and_descriptive_header(patients_first_dx, 'Patients First Diagnosis Date')\n",
    "    \n",
    "    save_file_and_folder(dataset_folder+'/reports/PatientsFirstDx.txt', report_body)\n",
    "    #patients_first_dx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_tables():\n",
    "    global loaded_tables_dict\n",
    "    print(\"\\nSTEP 2: Load all CSV tables.\")\n",
    "    for tableName in knownTables:\n",
    "        full_path = dataset_folder+'/raw_csv/'+tableName+'.csv'\n",
    "        if not os.path.exists(full_path):\n",
    "            print(\"WARN -- missing table \"+tableName)\n",
    "        else:\n",
    "            print('Reading ' + tableName+'.csv...')\n",
    "            df = pd.read_csv(full_path)\n",
    "            df = df.astype({\"PatientId\": str})\n",
    "            #df.set_index('PatientId', inplace=True)\n",
    "            #print(df.head(2))\n",
    "            loaded_tables_dict[tableName] = df\n",
    "            print(tableName +\" read.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_val(val, ifield:ImportField):\n",
    "    # print(\"enter process_val, name=\"+ifield.source_name+\",type=\"+ifield.type+\"!\")\n",
    "    if ifield.type=='NUMBER':  # for now, this means just integers\n",
    "        print(str(val))\n",
    "        try:\n",
    "            val = int(val)\n",
    "            if math.isnan(val):\n",
    "                print(\"nan found\")\n",
    "                val = \"\"\n",
    "            if str(val)==\"nan\":\n",
    "                print(\"nan text found\")\n",
    "                val = \"\"   \n",
    "        except:\n",
    "            val =\"\"\n",
    "    if ifield.type=='DATE':\n",
    "        # print(str(ifield.type)+\", val-type=\" + str(type(val)) + \" val=(\"+str(val)+\")\")\n",
    "        if isinstance(val, str):\n",
    "            val = datetime.strptime(val, date_format)\n",
    "            if str(val) == \"NaT\":  # Not a Time\n",
    "                val = \"\"\n",
    "        else:\n",
    "            val=\"\" # TBD: error reporting\n",
    "            print(\"WARN: expected string for date in \" +ifield.source_name+\", but isn't a string.\")\n",
    "    try:\n",
    "        #print(\"process_val about to return \"+str(val))\n",
    "        return val\n",
    "    except:\n",
    "        noval_list.append(pid)\n",
    "        return None\n",
    "\n",
    "\n",
    "# imports: a dictionary of column name from the tname table into patient table, where value is a function to convert data\n",
    "def import_to_patient_table(tname, import_fields:List[ImportField]): \n",
    "    global patients_first_dx, data_clinical_patient, loaded_tables_dict, current_lookup_field, noval_list\n",
    "    if (tname in loaded_tables_dict) == False:\n",
    "        print(\"WARN -- Could not process table \"+tname)\n",
    "    else:\n",
    "        current_table = loaded_tables_dict[tname]\n",
    "        for ifield in import_fields:\n",
    "\n",
    "            if ifield.source_name in current_table.columns:\n",
    "                data_clinical_patient.insert(1, ifield.final_name, None)\n",
    "               # global current_lookup_field\n",
    "                current_lookup_field= ifield.source_name\n",
    "                print(\"Looking for field \" + tname+\".\"+current_lookup_field)\n",
    "                noval_list.clear()\n",
    "\n",
    "                def get_field_value(pid):\n",
    "                    global current_lookup_field, noval_list\n",
    "                    gg = current_table.loc[current_table['PatientId'] == str(pid)]\n",
    "                    hh = gg[current_lookup_field]\n",
    "                    val = None\n",
    "                    try:\n",
    "                        val = hh.iloc[0] \n",
    "                        return process_val(val, ifield)\n",
    "\n",
    "                    except:\n",
    "                        #print(\"ERROR \"+current_lookup_field+\", \"+str(pid)+\"   \"+str(val)+\".\")\n",
    "                        #typef, value, traceback = sys.exc_info()\n",
    "                        #print('Error value '+ str (value))\n",
    "                        pass\n",
    "\n",
    "\n",
    "                new_values = data_clinical_patient['PatientId'].apply(get_field_value)\n",
    "                if len(noval_list) > 0 :\n",
    "                    percent_str = \"{0:.0%}\".format(len(noval_list) / new_values.shape[0])\n",
    "                    print(\"- Field \" + tname+\".\"+current_lookup_field + \" had \" + str(len(noval_list)) + \" missing entries. (\"+percent_str+\" empty)\")\n",
    "                    #print(str( len(noval_list) / new_values.shape[0]))\n",
    "\n",
    "\n",
    "                data_clinical_patient[ifield.final_name] = new_values\n",
    "                #print(data_clinical_patient.head(12))\n",
    "            else:\n",
    "                print(\"==== ERROR: Three is no column '\"+ifield.source_name+\"' in '\"+tname+\"' table. ====\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# event_table_name: name of table as it will appear in \"data_clinical_<timeline-foo>.txt\" filename. For events, should be \"timeline-foo\".\n",
    "# tname: Name of the dataframe table we have loaded from CSV, which contains these events to import.\n",
    "# import_fields: list of ImportFields OR str. str for case of EVENT_TYPE, which may not exist as column in an event CSV.\n",
    "\n",
    "# event_Type is the name of the subtype column (e.g., RADIATION)\n",
    "# subtype_source is the CSV column with the  source info for the subtype (e.g., RADTXTYPE)\n",
    "# safelist, if not None, is the *only* values acceptable for subtype.\n",
    "def import_to_event_table(event_table_name, tname, import_fields:List[ImportField], event_type=\"EVENT\", start_date_col_name=None, stop_date_col_name=None, subtype_source=None, graph_marker=None, safelist=None): \n",
    "    global patients_first_dx, data_clinical_patient, loaded_tables_dict, current_lookup_field, noval_list, has_custom_prep_file\n",
    "    print(\"INFO starting to import for event_type of....\")\n",
    "    print(str(event_type))\n",
    "    print(\"... .\")\n",
    "    if (tname in loaded_tables_dict) == False:\n",
    "        print(\"WARN -- Could not process table \"+tname)\n",
    "    else:\n",
    "        # strategy...\n",
    "        #   a_list = [['dog', 1], ['cat', 2], ['fish', 3]]\n",
    "        #   df = pd.DataFrame(a_list, columns=['animal', 'amount'])\n",
    "\n",
    "        current_table = loaded_tables_dict[tname]\n",
    "\n",
    "        # df_new_events = pd.DataFrame\n",
    "        # if  event_table_name not in data_clinical_timeline_dfs:\n",
    "        #     df_new_events = pd.DataFrame(columns=[\"PatientID\", \"START_DATE\", \"STOP_DATE\", \"EVENT_TYPE\"])\n",
    "        #     data_clinical_timeline_dfs[event_table_name] = df_new_events\n",
    "\n",
    "        these_subtype_rewrites = None  # for typos and cleanup\n",
    "        these_subtype_groupings = None # for bucketing for legend\n",
    "\n",
    "        noval_list.clear()\n",
    "        rows_to_save = []\n",
    "        column_names = [\"PatientId\", \"EVENT_TYPE\", \"START_DATE\", \"STOP_DATE\"]\n",
    "        for ifield in import_fields:\n",
    "            if ifield.source_name in current_table.columns:\n",
    "                column_names.append(ifield.final_name)\n",
    "        if subtype_source is not None:\n",
    "            column_names.append(event_type)  # e.g., \"RADIATION\"\n",
    "            if has_custom_prep_file:\n",
    "\n",
    "                if event_type not in custom_caisis_prep.subtype_rewrites:\n",
    "                    print(\"INFO: Event type '\"+event_type+\"' has no subtype rewrite rules.\")\n",
    "                else:\n",
    "                    these_subtype_rewrites = custom_caisis_prep.subtype_rewrites[event_type]\n",
    "                    print(\"INFO: Event type '\"+event_type+\"' has \"+ str(len(these_subtype_rewrites)) + \" subtype rewrite rules.\")\n",
    "\n",
    "\n",
    "                \n",
    "                if event_type not in custom_caisis_prep.subtype_groupings:\n",
    "                    print(\"INFO: Event type '\"+event_type+\"' has no subtype grouping rules.\")\n",
    "                else:\n",
    "                    these_subtype_groupings = custom_caisis_prep.subtype_groupings[event_type]\n",
    "                    print(\"INFO: Event type '\"+event_type+\"' has \"+ str(len(these_subtype_groupings)) + \" subtype grouping rules.\")\n",
    "            else:\n",
    "                print(\"In import_to_event_Table, has_custom_prep_file FALSE\")\n",
    "        print(\">>>>>>>>>>>\")\n",
    "        print('For ' + event_type+\", rewrite keys:\")\n",
    "        print(str(these_subtype_rewrites))\n",
    "        print('>>>end')\n",
    "        # Loop through rows in current_table.\n",
    "        # Pull out each field from import_fields, and add event type, and start and stop dates.\n",
    "        current_table_rows_of_dict = current_table.to_dict('records')\n",
    "        for row in current_table_rows_of_dict:\n",
    "\n",
    "            etype = None\n",
    "            if event_type in row:  # get event from the event_type column\n",
    "                etype = row[event_type]\n",
    "            else:  # just use event_type as text.\n",
    "                etype = event_type\n",
    "\n",
    "            # common to all events: pid, type, start, and stop.\n",
    "            pid = row['PatientId']\n",
    "            \n",
    "            start_date = missing_date\n",
    "            if start_date_col_name != None:\n",
    "                    start_date = row[start_date_col_name]\n",
    "            stop_date = missing_date\n",
    "            if stop_date_col_name != None:\n",
    "                stop_date = row[stop_date_col_name]\n",
    "\n",
    "            # print('pid='+pid+', etype='+etype+', start='+str(start_date)+',  stop='+str(stop_date)+'.')\n",
    "            \n",
    "            new_row = [pid, \n",
    "                process_val(etype, core_timeline_fields[0]), \n",
    "                process_val(start_date, core_timeline_fields[1]), \n",
    "                process_val(stop_date, core_timeline_fields[2]), \n",
    "                ]\n",
    "\n",
    "            print(\"columns....\")\n",
    "            print(*current_table.columns, sep = \", \") \n",
    "            print(\"done 123\")\n",
    "            if \"EncECOG_Score\" in current_table.columns:\n",
    "                print('YES, ECOG is there.')\n",
    "            else:\n",
    "                print(\"NO, ECOG is NOT there.\")\n",
    "\n",
    "            # now append all import fields\n",
    "            for ifield in import_fields:\n",
    "                if ifield.source_name in current_table.columns:\n",
    "                    fieldval = row[ifield.source_name]\n",
    "                    if ifield.source_name == \"EncECOG_Score\":\n",
    "                        print(\"===EncECOG_Score Seen!===\")\n",
    "                        print('if source_name='+ifield.source_name+',  fieldval = ' + str(fieldval))\n",
    "                    # process raw value\n",
    "                    val = None\n",
    "                    try:\n",
    "                        val = process_val(fieldval, ifield)\n",
    "                        if ifield.source_name == \"EncECOG_Score\":\n",
    "                            print('try source_name='+ifield.source_name+',  process_val = ' + str(fieldval))\n",
    "\n",
    "                    except:\n",
    "                        print(\"ERROR fiif  \"+current_lookup_field+\", \"+str(pid)+\"   \"+str(val)+\".\")\n",
    "                        typef, value, traceback = sys.exc_info()\n",
    "                        print('Error value '+ str (value))\n",
    "                        pass\n",
    "\n",
    "                    new_row.append(val)\n",
    "            print(\"new_row....\")\n",
    "            print(*new_row, sep = \", \")\n",
    "\n",
    "            if subtype_source is not None:\n",
    "                # print('wwww subtype_source is...')\n",
    "                # print(subtype_source)\n",
    "                # print(str(row))\n",
    "                # print('val...')\n",
    "                val = str(row[subtype_source]).strip()\n",
    "                if these_subtype_rewrites:\n",
    "                    key = val.lower()\n",
    "                    if key in these_subtype_rewrites:\n",
    "                        val = these_subtype_rewrites[key]\n",
    "                        # print(\"Rewrite Match: \"+key+\" -> \" + val)\n",
    "\n",
    "                if safelist is not None:\n",
    "                    if val not in safelist:\n",
    "                        # print('safelist does not contain ['+val+'].')\n",
    "                        continue\n",
    "\n",
    "                if these_subtype_groupings:\n",
    "                    lower_val = val.lower()\n",
    "                    grouping_match_found = False\n",
    "                    for regex in these_subtype_groupings:\n",
    "                        if re.search(regex, lower_val, re.IGNORECASE) is not None:\n",
    "                            val = these_subtype_groupings[regex]\n",
    "                            # print(\"Grouping Match: \"+ regex +\" -> \" + val)\n",
    "                            grouping_match_found = True\n",
    "                            break\n",
    "                    if grouping_match_found == False:\n",
    "                        val = \"Other \" + event_type\n",
    "\n",
    "                if val == \"-1234\":\n",
    "                    print(\"=====1234====\")\n",
    "\n",
    "                if (val.strip() == \"\") or (val.lower()==\"nan\"):\n",
    "                    val = \"MISSING\" # TBD: add reporting of error.\n",
    "                    print(\"WARNING: missing subtype value.\")\n",
    "                new_row.append(val)  #row[subtype_source])\n",
    "\n",
    "            # print(new_row)\n",
    "            rows_to_save.append(new_row)\n",
    "\n",
    "        # # turn list into df.\n",
    "        # new_values = data_clinical_patient['PatientId'].apply(get_field_value)\n",
    "        # data_clinical_patient[ifield.final_name] = new_values\n",
    "        # print(data_clinical_patient.head(12))\n",
    "\n",
    "\n",
    "        print('column_names....')\n",
    "        print(type(column_names))\n",
    "        print(column_names)\n",
    "        print('---- end of column_names ---')\n",
    "\n",
    "        added_df = pd.DataFrame(rows_to_save, columns=column_names)\n",
    "        data_clinical_timeline_dfs[event_table_name] = added_df\n",
    "\n",
    "        data_clinical_timeline_graph_markers[event_table_name] = graph_marker\n",
    "        \n",
    "        # if len(noval_list) > 0 :\n",
    "        #     percent_str = \"{0:.0%}\".format(len(noval_list) / new_values.shape[0])\n",
    "        #     print(\"- Field \" + tname+\".\"+current_lookup_field + \" had \" + str(len(noval_list)) + \" missing entries. (\"+percent_str+\" empty)\")\n",
    "        #     #print(str( len(noval_list) / new_values.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import LAST_DATE of given field in another table.\n",
    "# given table T, dataval field F, and dateval field D. Like largest value in dateval_field \"StatusDate\", for matching_val \"Alive\" in import_field.source_name \"Status\". (Then, Transform)\n",
    "global_gg = []\n",
    "\n",
    "def import_lastdate_to_patient_table(tname, import_fields:List[ImportField], dateval_field, dataval_field, matching_val): \n",
    "    global patients_first_dx, data_clinical_patient, loaded_tables_dict, current_lookup_field, noval_list\n",
    "    if (tname in loaded_tables_dict) == False:\n",
    "        print(\"WARN -- Could not process lastdate for table \"+tname)\n",
    "    else:\n",
    "        current_table = loaded_tables_dict[tname]\n",
    "        for ifield in import_fields: # really only one field here.\n",
    "\n",
    "            if ifield.source_name in current_table.columns:\n",
    "                data_clinical_patient.insert(1, ifield.final_name, None)\n",
    "                current_lookup_field= ifield.source_name\n",
    "                print(\"Looking for matching_val [\"+matching_val+\"] tbl \" + tname+\".\"+current_lookup_field)\n",
    "                noval_list.clear()\n",
    "\n",
    "                def get_field_value(pid):\n",
    "                    global current_lookup_field, noval_list, global_gg\n",
    "\n",
    "                    gg = current_table.loc[ (current_table['PatientId'] == str(pid)) & (current_table[dataval_field] == str(matching_val))  ]\n",
    "                    last_alive_str = gg[dateval_field].values[-1]  # <-- TBD: SHould actually loop through dates trings for last date. We *might* get an out of order Alive statement, so don't assume last in list is always the latest.\n",
    "                    val = None\n",
    "                    try:\n",
    "                        val = last_alive_str \n",
    "                        val_final = process_val(val, ifield)\n",
    "                        return val_final\n",
    "                    except:\n",
    "                        print(\"ERROR \"+current_lookup_field+\", \"+str(pid)+\"   \"+str(val)+\".\")\n",
    "                        #typef, value, traceback = sys.exc_info()\n",
    "                        #print('Error value '+ str (value))\n",
    "                        pass\n",
    "\n",
    "                new_values = data_clinical_patient['PatientId'].apply(get_field_value)\n",
    "                if len(noval_list) > 0 :\n",
    "                    percent_str = \"{0:.0%}\".format(len(noval_list) / new_values.shape[0])\n",
    "                    print(\"- Field \" + tname+\".\"+current_lookup_field + \" had \" + str(len(noval_list)) + \" missing entries. (\"+percent_str+\" empty)\")\n",
    "\n",
    "                data_clinical_patient[ifield.final_name] = new_values\n",
    "\n",
    "            else:\n",
    "                print(\"==== ERROR: There is no column '\"+ifield.source_name+\"' in '\"+tname+\"' table. Attempting lastdate. ====\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clinical_patient\n",
    "global_gg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_fields_to_patient_table():\n",
    "    # This is the core of specifying which fields we want to import. May change between datasets, so consider moving to an external file.\n",
    "    \n",
    "    print(\"\\nSTEP 3: Import columns to patient table.\")\n",
    "\n",
    "    import_fields = [\n",
    "       ImportField('PtGender', final_name='Sex') ,   #, conversion_function=None),\n",
    "       ImportField('PtBirthDate', final_name='BirthDate', type=\"DATE\" ),\n",
    "       ImportField('PtDeathDate', final_name='DeathDate', type=\"DATE\" )\n",
    "    ]\n",
    "    import_to_patient_table('Demographics', import_fields)\n",
    "    datafiles_fields['patient'].extend(import_fields)\n",
    "\n",
    "   \n",
    "    import_fields = [\n",
    "        ImportField('Status', final_name='LastAliveDate', type=\"DATE\") #, conversion_function=None )\n",
    "    ]\n",
    "    import_lastdate_to_patient_table('Status', import_fields, dataval_field=\"Status\", dateval_field=\"StatusDate\", matching_val=\"Alive\")\n",
    "    datafiles_fields['patient'].extend(import_fields)\n",
    "    \n",
    "\n",
    "    import_fields = [\n",
    "       ImportField('SocHxTobaccoType', final_name='Tobacco_Use'),\n",
    "       ImportField('SocHxTobaccoYears', final_name='Tobacco_Years' ), #SocHxAlcohol\n",
    "       ImportField('SocHxAlcohol', final_name='Alcohol_Use')\n",
    "    ]\n",
    "    import_to_patient_table('Social History', import_fields)\n",
    "    datafiles_fields['patient'].extend(import_fields)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_fields_to_event_tables():\n",
    "    # This is the core of specifying which fields we want to import. May change between datasets, so consider moving to an external file.\n",
    "\n",
    "    print(\"\\nSTEP 4: Import to events tables.\")\n",
    "    import_fields = [\n",
    "        ImportField('EncECOG_Score', final_name='ECOG', type='NUMBER') ,   #, conversion_function=None),\n",
    "        ImportField('EncKPS', final_name='KPS', type='NUMBER') ,   #, conversion_function=None),\n",
    "    ]\n",
    "    import_to_event_table('timeline-encounters', 'Encounters', import_fields, event_type='ENCOUNTERS', start_date_col_name='EncDate', stop_date_col_name='EncDate', graph_marker=graph_marker_types[\"diamonds\"] )\n",
    "    datafiles_fields['timeline-encounters'] = core_timeline_fields.copy()\n",
    "    datafiles_fields['timeline-encounters'].extend(import_fields)\n",
    "\n",
    "\n",
    "    print('datafiles_fields[timeline-encounters]  is....')\n",
    "    print(datafiles_fields['timeline-encounters'] )\n",
    "\n",
    "    import_fields = [\n",
    "        ImportField('RadTxType', final_name='RadTxType' ) ,   \n",
    "        ImportField('RadTxTarget', final_name='RadTxTarget' ) ,   \n",
    "        ImportField('RadTxTotalDose', final_name='RadTxTotalDose' ) ,   \n",
    "    ]\n",
    "    import_to_event_table('timeline-radiation', 'Radiation', import_fields, event_type='RADIATION', start_date_col_name='RadTxDate', stop_date_col_name='RadTxStopDate', subtype_source=\"RadTxType\", graph_marker=graph_marker_types[\"arcs\"] )\n",
    "    datafiles_fields['timeline-radiation'] = core_timeline_fields.copy()\n",
    "    datafiles_fields['timeline-radiation'].extend(import_fields)\n",
    "    datafiles_fields['timeline-radiation'].extend( [ ImportField('RADIATION', final_name='RADIATION') ])\n",
    "    \n",
    "    import_fields = [\n",
    "        ImportField('RadTxType', final_name='RadTxType' ) ,   \n",
    "        ImportField('RadTxTarget', final_name='RadTxTarget' ) ,   \n",
    "        ImportField('RadTxTotalDose', final_name='RadTxTotalDose' ) ,   \n",
    "    ]\n",
    "    import_to_event_table('timeline-medicaltherapy', 'Medical Therapy', import_fields, event_type='MEDICALTHERAPY', start_date_col_name='MedTxDate', stop_date_col_name='MedTxStopDate', subtype_source=\"MedTxAgent\" )\n",
    "    datafiles_fields['timeline-medicaltherapy'] = core_timeline_fields.copy()\n",
    "    datafiles_fields['timeline-medicaltherapy'].extend(import_fields)\n",
    "    datafiles_fields['timeline-medicaltherapy'].extend( [ ImportField('MEDICALTHERAPY', final_name='MEDICALTHERAPY') ])\n",
    "    \n",
    "    import_fields = [\n",
    "        # ImportField('Status', final_name='Status' ) ,   \n",
    "    ]\n",
    "    import_to_event_table('timeline-status', 'Status', import_fields, event_type='STATUS', start_date_col_name='StatusDate', stop_date_col_name='StatusDate', subtype_source=\"Status\", safelist=[\n",
    "        '1st Progression',\n",
    "        '2nd Progression',\n",
    "        '3rd Progression',\n",
    "        'Last Status Check',\n",
    "        'Recurrence',\n",
    "        'Last Status Check',\n",
    "        'Local Recurrence',\n",
    "        'Locoregional',\n",
    "        'Metastatic Disease',\n",
    "        'New Diagnosis',\n",
    "        'Newly Diagnosed',\n",
    "        'No Evidence of Disease',\n",
    "\n",
    "        ] )\n",
    "    datafiles_fields['timeline-status'] = core_timeline_fields.copy()\n",
    "    datafiles_fields['timeline-status'].extend(import_fields)\n",
    "    datafiles_fields['timeline-status'].extend( [ ImportField('STATUS', final_name='STATUS') ])\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_col_types(filename):\n",
    "    rr=reversed(list(map(lambda s: s.type, datafiles_fields[filename])))  # ????? TBD: reversed?\n",
    "    q=[]\n",
    "    # if filename=='patient':\n",
    "    q.append('STRING') # for 'PatientID'\n",
    "    if 'timeline-' in filename:\n",
    "        # rr2=list(map(lambda s: s, datafiles_fields[filename]))\n",
    "        # for w in list(rr2):\n",
    "        #     print( \"sourcename> \"+str(w.source_name))\n",
    "\n",
    "        wq=list(map(lambda s: s.type, datafiles_fields[filename]))\n",
    "        print(\"in getcoltypes for \"+filename+\",...\")\n",
    "        for atype in list(wq):\n",
    "            print( \"> \"+str(atype))\n",
    "            q.append(atype)\n",
    "    else:\n",
    "        # just patient table\n",
    "        q.extend(list(rr))\n",
    "\n",
    "    if filename=='patient':\n",
    "        q.append('DATE') # for 'DiagnosisDate'\n",
    "\n",
    "    print(\"end getcoltypes,\")\n",
    "    print(q)\n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_file_as_cbioportal(df:pd.DataFrame, filename):\n",
    "    output_filename = 'data_clinical_'+filename+'.txt'\n",
    "    full_filename = dataset_folder + \"/\"+foldername_with_headers+\"/\" + output_filename\n",
    "    if not os.path.exists(os.path.dirname(full_filename)):\n",
    "        try:\n",
    "            os.makedirs(os.path.dirname(full_filename))\n",
    "        except OSError as exc: # Guard against race condition\n",
    "            if exc.errno != errno.EEXIST:\n",
    "                raise\n",
    "\n",
    "    with open(full_filename, 'w', newline='\\r\\n') as f:\n",
    "        col_names_raw = list(df.columns.values)   # was data_clinical_patient.columns.values\n",
    "        col_names_raw[0] = \"PATIENT_ID\" # Hack to insert underscore\n",
    "        print('col_names_raw...')\n",
    "        print(col_names_raw)\n",
    "\n",
    "        col_names = [x.upper() for x in col_names_raw]\n",
    "        col_types = get_col_types(filename)\n",
    "        \n",
    "        f.write('#' + '\\t'.join(col_names) + \"\\n\") # header 1, internal name\n",
    "        f.write('#' + '\\t'.join(col_names) + \"\\n\")  # header 2, description\n",
    "        f.write('#' + '\\t'.join(col_types) + \"\\n\")  # header 3, type (STRING or NUMBER)  <<<<<\n",
    "        f.write('#' + '\\t'.join(['1'] * len(col_names)) + \"\\n\")  # header 4, position\n",
    "        f.write('\\t'.join(col_names) + \"\\n\")  # header 5, readable name\n",
    "\n",
    "        for row in df.iterrows():\n",
    "            output_row = []\n",
    "            row_as_list = list(row[1])\n",
    "            i=0\n",
    "\n",
    "            for item in row_as_list:\n",
    "                cleaned_item = str(item)\n",
    "                # print('--> '+cleaned_item)\n",
    "                if item == None:\n",
    "                    cleaned_item = \"\"\n",
    "                else:\n",
    "                    if 'time' in str(type(item)):   # datetime.datetime or pandas...timestamp.Timestamp # this does not work:  col_types[i]==\"DATE\":  # \n",
    "                        # print('DATE for col ' + str(i) +', '+str(col_names[i]))\n",
    "                        cleaned_item = datetime.strftime(item, date_format) # '%Y-%M-%d')\n",
    "                if (str(item)==\"nan\"):\n",
    "                    cleaned_item = \"\"\n",
    "                output_row.append(cleaned_item)\n",
    "                i=i+1\n",
    "\n",
    "            items_to_str = '\\t'.join(output_row)\n",
    "            f.write(items_to_str + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_empty_folder(dir):\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "        print(\"Created Directory : \", dir)\n",
    "    else:\n",
    "        print(\"Directory already existed : \", dir)  \n",
    "        import shutil\n",
    "        for root, dirs, files in os.walk(dir):\n",
    "            for f in files:\n",
    "                os.unlink(os.path.join(root, f))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_files_as_cbioportal():\n",
    "    # clear out the folder, if it has files\n",
    "    dir = dataset_folder + \"/\"+foldername_with_headers+\"/\" \n",
    "    create_and_empty_folder(dir)\n",
    "\n",
    "    print('Write patient meta file...')\n",
    "\n",
    "    filename = 'patient'\n",
    "    write_file_as_cbioportal(data_clinical_patient, filename)\n",
    "    \n",
    "    # loop through all table files\n",
    "    for df_key in data_clinical_timeline_dfs.keys():\n",
    "        print('Attempt to write '+df_key+'.')\n",
    "        write_file_as_cbioportal( data_clinical_timeline_dfs[df_key], df_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_patients_first_dx()\n",
    "print(patients_first_dx.shape)\n",
    "load_all_tables()\n",
    "datafiles_fields = {\n",
    "    'patient': []   #not including PATIENT_ID\n",
    "}\n",
    "import_fields_to_patient_table()\n",
    "import_fields_to_event_tables()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_files_as_cbioportal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_meta_files(dir):\n",
    "    # Assumes dir already created, with create_and_empty_folder().\n",
    "\n",
    "    full_filename = dir+\"/\" + \"meta_study.txt\"\n",
    "    with open(full_filename, 'w', newline='\\r\\n') as f:\n",
    "        f.write(\"type_of_cancer: \"+ type_of_cancer +\"\\n\") \n",
    "        f.write(\"cancer_study_identifier: \"+ study_identifier +\"\\n\" ) \n",
    "        f.write(\"name: \"+ study_identifier +\"\\n\") \n",
    "        f.write(\"short_name: \"+ study_identifier +\"\\n\") \n",
    "        f.write(\"description: \"+ study_description +\"\\n\") \n",
    "        f.write(\"add_global_case_list: true\" +\"\\n\") \n",
    "\n",
    "    full_filename = dir+\"/\" + \"meta_clinical_patient.txt\"\n",
    "    with open(full_filename, 'w', newline='\\r\\n') as f:\n",
    "        f.write(\"cancer_study_identifier: \"+ study_identifier +\"\\n\" ) \n",
    "        f.write(\"genetic_alteration_type: CLINICAL\\n\") \n",
    "        f.write(\"datatype: PATIENT_ATTRIBUTES\\n\") \n",
    "        f.write(\"data_filename: data_clinical_patient.txt\" +\"\\n\") \n",
    "\n",
    "    # ==== START SAMPLE FILE =====\n",
    "    # Oncoscape expects a sample/specimen file. Currently we don't support importing them, \n",
    "    # so for now make a dummy file with a one-to-one mapping of patientid with \"sample-<patientid>\".\n",
    "    full_filename = dir+\"/\" + \"meta_clinical_specimen_placeholder.txt\"\n",
    "    with open(full_filename, 'w', newline='\\r\\n') as f:\n",
    "        f.write(\"cancer_study_identifier: \"+ study_identifier +\"\\n\" ) \n",
    "        f.write(\"genetic_alteration_type: CLINICAL\\n\") \n",
    "        f.write(\"datatype: SAMPLE_ATTRIBUTES\\n\") \n",
    "        f.write(\"data_filename: data_clinical_specimen_placeholder.txt\" +\"\\n\") \n",
    "\n",
    "    full_filename = dir+\"/\" + \"data_clinical_specimen_placeholder.txt\"\n",
    "    with open(full_filename, 'w', newline='\\r\\n') as f:\n",
    "        f.write(\"#Unique_patient_identifier\tSPECIMEN_ID\\n\")\n",
    "        f.write(\"#STRING\tSTRING\\n\")\n",
    "        f.write(\"#1\t1\\n\")\n",
    "        f.write(\"PATIENT_ID\tSPECIMEN_ID\\n\")\n",
    "        for row in data_clinical_patient.iterrows():\n",
    "            f.write(row[1]['PatientId'] +\"\\tsample-\" + row[1]['PatientId']+\"\\n\")\n",
    "    # ==== END SAMPLE FILE =====\n",
    "\n",
    "    for event_table_name in data_clinical_timeline_dfs.keys():\n",
    "        full_filename = dir+\"/\" + \"meta_clinical_\"+event_table_name+\".txt\"\n",
    "        with open(full_filename, 'w', newline='\\r\\n') as f:\n",
    "            f.write(\"cancer_study_identifier: \"+ study_identifier +\"\\n\" ) \n",
    "            f.write(\"genetic_alteration_type: CLINICAL\\n\") \n",
    "            f.write(\"datatype: TIMELINE\\n\") \n",
    "            f.write(\"data_filename: data_clinical_\"+ event_table_name+\".txt\" +\"\\n\") \n",
    "            graph_marker = data_clinical_timeline_graph_markers[event_table_name]\n",
    "            if graph_marker is None:\n",
    "                # f.write('oncoscape_bar_override: {\"version\": \"1.0\", \"style\": \"Bars\", \"shape\": \"circle\", \"subtypeColors\": {\"rp\": \"#FF0000\", \"xrt\": \"#00FF00\"} }')\n",
    "                pass\n",
    "            else:\n",
    "                label = event_table_name\n",
    "                if label.startswith('timeline-'):\n",
    "                    label = label[len('timeline-'):]\n",
    "\n",
    "                bar_override = 'oncoscape_bar_override: {\"version\": \"1.0\", \"label\": \"'+label+'\" '\n",
    "                shape_extender = \"\"\n",
    "                print(\"===========  graph_marker_key ===========\")\n",
    "                print(str(graph_marker))\n",
    "                if graph_marker[\"shape\"] is not None:\n",
    "                    shape_extender = ', \"shape\": \"'+graph_marker[\"shape\"]+'\" '\n",
    "                bar_override = bar_override + ', \"style\": \"'+graph_marker[\"mtype\"]+'\" ' + shape_extender\n",
    "                bar_override = bar_override + ' }'\n",
    "                \n",
    "\n",
    "                f.write(bar_override)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = dataset_folder + \"/\"+foldername_zero_dates+\"/\" \n",
    "create_and_empty_folder(dir)\n",
    "zero_dates(dataset_folder+'/'+foldername_with_headers, patient_info_filename = \"data_clinical_patient.txt\", zero_day_column_name =  \"DIAGNOSISDATE\", output_folder=dir )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_meta_files(dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
