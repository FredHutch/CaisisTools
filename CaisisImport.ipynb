{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CaisisImport. A utility to prepare a Caisis-to-Excel export for import into Oncoscape (via cBioPortal format).\n",
    "\n",
    "# Steps:\n",
    "# . Limit by disease\n",
    "# . ExportAsTSVs\n",
    "# . ZeroDates\n",
    "# . \n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import errno\n",
    "import sys\n",
    "import math\n",
    "import pipes\n",
    "#import datetime\n",
    "from typing import List\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "\n",
    "dataset_folder = '../Caisis_NonPublicData/Sarcoma_nov2021'\n",
    "diseaseChosen = 'Sarcoma'\n",
    "## knownTables = ['Demographics', 'Encounters', 'ClinicalStage', 'LabTests', 'PathTest', 'SocialHistory', 'LabTestGenetics', 'PathStageGrade', 'RadiationTherapy',  ]\n",
    "# table Status is treated separately, first.\n",
    "knownTables = ['Clinical Stages', 'Demographics', 'Encounters', 'Medical Therapy', 'Pathology', 'Procedures', 'Radiation', 'Social History']\n",
    "date_format = '%Y-%M-%d'  # '%d-%b-%y'   #'%b %d %Y %I:%M%p'\n",
    "#days_before_dx_to_include = 0  # if 30, can include events up to 30 days before the diagnosis date.\n",
    "missing_date = datetime(2222, 2,2)  # use instead of empty/None to indicate missing data\n",
    "\n",
    "## -- internal --\n",
    "patients_first_dx = pd.DataFrame()   # just PatientId and DiagnosisDate\n",
    "data_clinical_patient = pd.DataFrame() # Will become the data_clinical_patient table.\n",
    "loaded_tables_dict = {}\n",
    "data_clinical_patient = None\n",
    "data_clinical_timeline_dfs = {}   # dictionary of dataframes, keyed off of \"timeline-foo\" names.\n",
    "datafiles_fields = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImportField:\n",
    "    source_name:str\n",
    "    final_name:str\n",
    "    type:str = 'STRING'  # STRING, NUMBER, or DATE (DATE gets turned into STING in final TSV files.)\n",
    "    conversion_function = None\n",
    "\n",
    "    def __init__(self, source_name, final_name, type=\"STRING\", conversion_function=None ):\n",
    "        self.source_name = source_name\n",
    "        self.final_name = final_name\n",
    "        self.type = type\n",
    "        self.conversion_function = conversion_function\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.source_name +\"->\"+ self.final_name+\", type=\"+self.type+\".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_file_and_folder(filename, obj):\n",
    "    if not os.path.exists(os.path.dirname(filename)):\n",
    "        try:\n",
    "            os.makedirs(os.path.dirname(filename))\n",
    "        except OSError as exc: # Guard against race condition\n",
    "            if exc.errno != errno.EEXIST:\n",
    "                raise\n",
    "\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(obj)\n",
    "\n",
    "def patients_and_descriptive_header(df:pd.DataFrame, header):\n",
    "    # TBD: note header in separate place\n",
    "    str_ids = df.to_string(index=False)\n",
    "    a =  str_ids  # header + \"\\n\" + str_ids\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_patients_first_dx():\n",
    "    global patients_first_dx, data_clinical_patient, dataset_folder\n",
    "    print(\"\\nSTEP 1: Find patients' first diagnosis date.\")\n",
    "    print(dataset_folder)\n",
    "    status_fullpath = dataset_folder +'/raw_csv/Status.csv'\n",
    "    if not os.path.exists(dataset_folder +'/raw_csv'):\n",
    "        sys.exit('ERROR: Cannot find folder \"raw_csv\".')\n",
    "\n",
    "    if not os.path.exists(status_fullpath):\n",
    "        sys.exit('ERROR: Status.csv file not found at ' + status_fullpath)\n",
    "\n",
    "    tbl_status = pd.read_csv(status_fullpath) #'Demographics.csv')\n",
    "    tbl_status['Date'] =pd.to_datetime(tbl_status.StatusDate)\n",
    "    tbl_status['PatientId'] =  tbl_status['PatientId'].astype(str)\n",
    "\n",
    "    dxRows = tbl_status[tbl_status.Status.eq('Diagnosis Date')]\n",
    "    diseaseRows = dxRows[dxRows.StatusDisease.eq(diseaseChosen)].sort_values('Date')\n",
    "\n",
    "    dxNanRows = diseaseRows[diseaseRows.Date.isna()][['PatientId']]\n",
    "    if(dxNanRows.size > 0):\n",
    "        print('CHECK NoDxDate? REPORT NoDxDate.txt has *' +str(dxNanRows.size)+'* diagnoses of '+diseaseChosen+' without diagnosis dates.')\n",
    "        report_body = patients_and_descriptive_header(dxNanRows, 'Patients with \"'+diseaseChosen+\" but no DiagnosisDate:\")\n",
    "    #    save_file_and_folder('reports/NoDxDate.txt', report_body)\n",
    "        save_file_and_folder(dataset_folder+'/reports/NoDxDate.txt', report_body)\n",
    "    else:\n",
    "        print('CHECK NoDxDate? OK')\n",
    "        print('TBD: delete existing NoDxDate.txt report.')\n",
    "        if os.path.exists(dataset_folder+'/reports/NoDxDate.txt'):\n",
    "            os.remove(dataset_folder+'/reports/NoDxDate.txt')\n",
    "            print('REMOVED NoDxDate.txt')\n",
    "            \n",
    "    diseaseDatedRows = diseaseRows[diseaseRows.Date.isna()==False]\n",
    "    patientid_date_dict = {}\n",
    "    for index, row in diseaseDatedRows.iterrows():\n",
    "        pid = str(row['PatientId'])\n",
    "        if((pid in patientid_date_dict) == False):\n",
    "            patientid_date_dict[pid] = row['Date']\n",
    "        else:\n",
    "            pass\n",
    "    print('Resulting patient IDs = ' + str(len(patientid_date_dict)))   \n",
    "\n",
    "    data = []\n",
    "    for key in patientid_date_dict.keys():\n",
    "        new_row = [key, patientid_date_dict[key]]\n",
    "        data.append(new_row)\n",
    "    patients_first_dx = pd.DataFrame(data, columns=[\"PatientId\", \"DiagnosisDate\"])\n",
    "    data_clinical_patient = patients_first_dx.copy()\n",
    "\n",
    "    report_body = patients_and_descriptive_header(patients_first_dx, 'Patients First Diagnosis Date')\n",
    "    \n",
    "    save_file_and_folder(dataset_folder+'/reports/PatientsFirstDx.txt', report_body)\n",
    "    #patients_first_dx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_tables():\n",
    "    global loaded_tables_dict\n",
    "    print(\"\\nSTEP 2: Load all CSV tables.\")\n",
    "    for tableName in knownTables:\n",
    "        full_path = dataset_folder+'/raw_csv/'+tableName+'.csv'\n",
    "        if not os.path.exists(full_path):\n",
    "            print(\"WARN -- missing table \"+tableName)\n",
    "        else:\n",
    "            print('Reading ' + tableName+'.csv...')\n",
    "            df = pd.read_csv(full_path)\n",
    "            df = df.astype({\"PatientId\": str})\n",
    "            #df.set_index('PatientId', inplace=True)\n",
    "            #print(df.head(2))\n",
    "            loaded_tables_dict[tableName] = df\n",
    "            print(tableName +\" read.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_lookup_field = None  # placeholder.\n",
    "noval_list = []   # List of patient IDs with no associated value in one or more of the import_fields. Use this for reporting. \n",
    "\n",
    "def process_val(val, ifield:ImportField):\n",
    "    if ifield.type=='NUMBER':\n",
    "        val = val # convert to number\n",
    "        if math.isnan(val):\n",
    "            print(\"nan found\")\n",
    "            val = \"\"\n",
    "        if str(val)==\"nan\":\n",
    "            print(\"nan text found\")\n",
    "            val = \"\"            \n",
    "    if ifield.type=='DATE':\n",
    "        # print(str(ifield.type)+\", val...\"+str(val))\n",
    "        if isinstance(val, str):\n",
    "            val = datetime.strptime(val, date_format)\n",
    "        else:\n",
    "            val=\"NOTSTR\" # TBD: error reporting\n",
    "    try:\n",
    "        return val\n",
    "    except:\n",
    "        noval_list.append(pid)\n",
    "        return None\n",
    "\n",
    "\n",
    "# imports: a dictionary of column name from the tname table into patient table, where value is a function to convert data\n",
    "def import_to_patient_table(tname, import_fields:List[ImportField]): \n",
    "    global patients_first_dx, data_clinical_patient, loaded_tables_dict, current_lookup_field, noval_list\n",
    "    if (tname in loaded_tables_dict) == False:\n",
    "        print(\"WARN -- Could not process table \"+tname)\n",
    "    else:\n",
    "        current_table = loaded_tables_dict[tname]\n",
    "        for ifield in import_fields:\n",
    "\n",
    "            if ifield.source_name in current_table.columns:\n",
    "                data_clinical_patient.insert(1, ifield.final_name, None)\n",
    "               # global current_lookup_field\n",
    "                current_lookup_field= ifield.source_name\n",
    "                print(\"Looking for field \" + tname+\".\"+current_lookup_field)\n",
    "                noval_list.clear()\n",
    "\n",
    "                def get_field_value(pid):\n",
    "                    global current_lookup_field, noval_list\n",
    "                    gg = current_table.loc[current_table['PatientId'] == str(pid)]\n",
    "                    hh = gg[current_lookup_field]\n",
    "                    val = None\n",
    "                    try:\n",
    "                        val = hh.iloc[0] \n",
    "                        return process_val(val, ifield)\n",
    "\n",
    "                    except:\n",
    "                        #print(\"ERROR \"+current_lookup_field+\", \"+str(pid)+\"   \"+str(val)+\".\")\n",
    "                        #typef, value, traceback = sys.exc_info()\n",
    "                        #print('Error value '+ str (value))\n",
    "                        pass\n",
    "\n",
    "\n",
    "                new_values = data_clinical_patient['PatientId'].apply(get_field_value)\n",
    "                if len(noval_list) > 0 :\n",
    "                    percent_str = \"{0:.0%}\".format(len(noval_list) / new_values.shape[0])\n",
    "                    print(\"- Field \" + tname+\".\"+current_lookup_field + \" had \" + str(len(noval_list)) + \" missing entries. (\"+percent_str+\" empty)\")\n",
    "                    #print(str( len(noval_list) / new_values.shape[0]))\n",
    "\n",
    "\n",
    "                data_clinical_patient[ifield.final_name] = new_values\n",
    "                #print(data_clinical_patient.head(12))\n",
    "            else:\n",
    "                print(\"==== ERROR: Three is no column '\"+ifield.source_name+\"' in '\"+tname+\"' table. ====\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# event_table_name: name of table as it will appear in \"data_clinical_<timeline-foo>.txt\" filename. For events, should be \"timeline-foo\".\n",
    "# tname: Name of the dataframe table we have loaded from CSV, which contains these events to import.\n",
    "# import_fields: list of ImportFields OR str. str for case of EVENT_TYPE, which may not exist as column in an event CSV.\n",
    "def import_to_event_table(event_table_name, tname, import_fields:List[ImportField], event_type=\"EVENT\", start_date_col_name=None, stop_date_col_name=None): \n",
    "    global patients_first_dx, data_clinical_patient, loaded_tables_dict, current_lookup_field, noval_list\n",
    "    if (tname in loaded_tables_dict) == False:\n",
    "        print(\"WARN -- Could not process table \"+tname)\n",
    "    else:\n",
    "        # strategy...\n",
    "        #   a_list = [['dog', 1], ['cat', 2], ['fish', 3]]\n",
    "        #   df = pd.DataFrame(a_list, columns=['animal', 'amount'])\n",
    "\n",
    "        current_table = loaded_tables_dict[tname]\n",
    "\n",
    "        # df_new_events = pd.DataFrame\n",
    "        # if  event_table_name not in data_clinical_timeline_dfs:\n",
    "        #     df_new_events = pd.DataFrame(columns=[\"PatientID\", \"START_DATE\", \"STOP_DATE\", \"EVENT_TYPE\"])\n",
    "        #     data_clinical_timeline_dfs[event_table_name] = df_new_events\n",
    "\n",
    "        noval_list.clear()\n",
    "        rows_to_save = []\n",
    "        column_names = [\"PatientId\", \"EVENT_TYPE\", \"START_DATE\", \"STOP_DATE\"]\n",
    "        for ifield in import_fields:\n",
    "            if ifield.source_name in current_table.columns:\n",
    "                column_names.append(ifield.final_name)\n",
    "\n",
    "        # Loop through rows in current_table.\n",
    "        # Pull out each field from import_fields, and add event type, and start and stop dates.\n",
    "        current_table_rows_of_dict = current_table.to_dict('records')\n",
    "        for row in current_table_rows_of_dict:\n",
    "            etype = None\n",
    "            if event_type in row:  # get event from the event_type column\n",
    "                etype = row[event_type]\n",
    "            else:  # just use event_type as text.\n",
    "                etype = event_type\n",
    "\n",
    "            # common to all events: pid, type, start, and stop.\n",
    "            pid = row['PatientId']\n",
    "            \n",
    "            start_date = missing_date\n",
    "            if start_date_col_name != None:\n",
    "                    start_date = row[start_date_col_name]\n",
    "            stop_date = missing_date\n",
    "            if stop_date_col_name != None:\n",
    "                stop_date = row[stop_date_col_name]\n",
    "\n",
    "            # print('pid='+pid+', etype='+etype+', start='+str(start_date)+',  stop='+str(stop_date)+'.')\n",
    "            \n",
    "            new_row = [pid, etype, start_date, stop_date]\n",
    "            # now append all import fields\n",
    "            for ifield in import_fields:\n",
    "                if ifield.source_name in current_table.columns:\n",
    "                    fieldval = row[ifield.source_name]\n",
    "                    #  print('source_name='+ifield.source_name+',  fieldval = ' + str(fieldval))\n",
    "                    # process raw value\n",
    "                    val = None\n",
    "                    try:\n",
    "                        val = process_val(fieldval, ifield)\n",
    "\n",
    "                    except:\n",
    "                        print(\"ERROR \"+current_lookup_field+\", \"+str(pid)+\"   \"+str(val)+\".\")\n",
    "                        typef, value, traceback = sys.exc_info()\n",
    "                        print('Error value '+ str (value))\n",
    "                        pass\n",
    "\n",
    "                    new_row.append(val)\n",
    "            # print(new_row)\n",
    "            rows_to_save.append(new_row)\n",
    "\n",
    "        # # turn list into df.\n",
    "        # new_values = data_clinical_patient['PatientId'].apply(get_field_value)\n",
    "        # data_clinical_patient[ifield.final_name] = new_values\n",
    "        # print(data_clinical_patient.head(12))\n",
    "\n",
    "\n",
    "        print('column_names....')\n",
    "        print(type(column_names))\n",
    "        print(column_names)\n",
    "        print('---- end of column_names ---')\n",
    "\n",
    "        added_df = pd.DataFrame(rows_to_save, columns=column_names)\n",
    "        data_clinical_timeline_dfs[event_table_name] = added_df\n",
    "        # if len(noval_list) > 0 :\n",
    "        #     percent_str = \"{0:.0%}\".format(len(noval_list) / new_values.shape[0])\n",
    "        #     print(\"- Field \" + tname+\".\"+current_lookup_field + \" had \" + str(len(noval_list)) + \" missing entries. (\"+percent_str+\" empty)\")\n",
    "        #     #print(str( len(noval_list) / new_values.shape[0]))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_fields_to_patient_table():\n",
    "    # This is the core of specifying which fields we want to import. May change between datasets, so consider moving to an external file.\n",
    "    \n",
    "    print(\"\\nSTEP 3: Import columns to patient table.\")\n",
    "    import_fields = [\n",
    "        ImportField('PtGender', final_name='Sex') ,   #, conversion_function=None),\n",
    "        ImportField('PtBirthDate', final_name='BirthDate', type=\"DATE\" )\n",
    "    ]\n",
    "    import_to_patient_table('Demographics', import_fields)\n",
    "    datafiles_fields['patient'].extend(import_fields)\n",
    "\n",
    "    import_fields = [\n",
    "        ImportField('SocHxTobaccoType', final_name='Tobacco_Use'),\n",
    "        ImportField('SocHxTobaccoYears', final_name='Tobacco_Years' ), #SocHxAlcohol\n",
    "        ImportField('SocHxAlcohol', final_name='Alcohol_Use')\n",
    "    ]\n",
    "    import_to_patient_table('Social History', import_fields)\n",
    "    datafiles_fields['patient'].extend(import_fields)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_timeline_fields = [\n",
    "    ImportField('EVENT_TYPE', final_name='EVENT_TYPE') , \n",
    "    ImportField('START_DATE', final_name='START_DATE', type='DATE') , \n",
    "    ImportField('STOP_DATE', final_name='STOP_DATE', type='DATE') , \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_fields_to_event_table():\n",
    "    # This is the core of specifying which fields we want to import. May change between datasets, so consider moving to an external file.\n",
    "\n",
    "    print(\"\\nSTEP 4: Import to events tables.\")\n",
    "    import_fields = [\n",
    "        ImportField('EncECOG', final_name='ECOG', type='NUMBER') ,   #, conversion_function=None),\n",
    "        ImportField('EncKPS', final_name='KPS') ,   #, conversion_function=None),\n",
    "    ]\n",
    "    import_to_event_table('timeline-encounters', 'Encounters', import_fields, event_type='ENCOUNTER', start_date_col_name='EncDate', stop_date_col_name='EncDate' )\n",
    "    datafiles_fields['timeline-encounters'] = core_timeline_fields.copy()\n",
    "    datafiles_fields['timeline-encounters'].extend(import_fields)\n",
    "\n",
    "    print('datafiles_fields[timeline-encounters]  is....')\n",
    "    print(datafiles_fields['timeline-encounters'] )\n",
    "\n",
    "    import_fields = [\n",
    "        ImportField('RadTxType', final_name='RadTxType' ) ,   \n",
    "        ImportField('RadTxTarget', final_name='RadTxTarget' ) ,   \n",
    "        ImportField('RadTxTotalDose', final_name='RadTxTotalDose' ) ,   \n",
    "    ]\n",
    "    import_to_event_table('timeline-radiation', 'Radiation', import_fields, event_type='RADIATION', start_date_col_name='RadTxDate', stop_date_col_name='RadTxStopDate' )\n",
    "    datafiles_fields['timeline-radiation'] = core_timeline_fields.copy()\n",
    "    datafiles_fields['timeline-radiation'].extend(import_fields)\n",
    "\n",
    "    print('datafiles_fields[timeline-radiation]  is....')\n",
    "    print(datafiles_fields['timeline-radiation'] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_col_types(filename):\n",
    "    rr=reversed(list(map(lambda s: s.type, datafiles_fields[filename])))  # ????? TBD: reversed?\n",
    "    q=[]\n",
    "    # if filename=='patient':\n",
    "    q.append('STRING') # for 'PatientID'\n",
    "    if 'timeline-' in filename:\n",
    "        # rr2=list(map(lambda s: s, datafiles_fields[filename]))\n",
    "        # for w in list(rr2):\n",
    "        #     print( \"sourcename> \"+str(w.source_name))\n",
    "\n",
    "        wq=list(map(lambda s: s.type, datafiles_fields[filename]))\n",
    "        print(\"in getcoltypes for \"+filename+\",...\")\n",
    "        for atype in list(wq):\n",
    "            print( \"> \"+str(atype))\n",
    "            q.append(atype)\n",
    "    else:\n",
    "        # just patient table\n",
    "        q.extend(list(rr))\n",
    "\n",
    "    if filename=='patient':\n",
    "        q.append('DATE') # for 'DiagnosisDate'\n",
    "\n",
    "    print(\"end getcoltypes,\")\n",
    "    print(q)\n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_file_as_cbioportal(df:pd.DataFrame, filename):\n",
    "    output_filename = 'data_clinical_'+filename+'.txt'\n",
    "    full_filename = dataset_folder + \"/01_with_headers/\" + output_filename\n",
    "    if not os.path.exists(os.path.dirname(full_filename)):\n",
    "        try:\n",
    "            os.makedirs(os.path.dirname(full_filename))\n",
    "        except OSError as exc: # Guard against race condition\n",
    "            if exc.errno != errno.EEXIST:\n",
    "                raise\n",
    "\n",
    "    with open(full_filename, \"w\") as f:\n",
    "        col_names_raw = list(df.columns.values)   # was data_clinical_patient.columns.values\n",
    "        col_names = [x.upper() for x in col_names_raw]\n",
    "        col_types = get_col_types(filename)\n",
    "        \n",
    "        f.write('#' + '\\t'.join(col_names) + \"\\n\") # header 1, internal name\n",
    "        f.write('#' + '\\t'.join(col_names) + \"\\n\")  # header 2, description\n",
    "        f.write('#' + '\\t'.join(col_types) + \"\\n\")  # header 3, type (STRING or NUMBER)  <<<<<\n",
    "        f.write('#' + '\\t'.join(['1'] * len(col_names)) + \"\\n\")  # header 4, position\n",
    "        f.write('\\t'.join(col_names) + \"\\n\")  # header 5, readable name\n",
    "\n",
    "        for row in df.iterrows():\n",
    "            output_row = []\n",
    "            row_as_list = list(row[1])\n",
    "            i=0\n",
    "\n",
    "            for item in row_as_list:\n",
    "                cleaned_item = str(item)\n",
    "                if item == None:\n",
    "                    cleaned_item = \"\"\n",
    "                else:\n",
    "                    if 'time' in str(type(item)):   # datetime.datetime or pandas...timestamp.Timestamp # this does not work:  col_types[i]==\"DATE\":  # \n",
    "                        # print('DATE for col ' + str(i) +', '+str(col_names[i]))\n",
    "                        cleaned_item = str(datetime.strftime(item, '%Y-%M-%d'))\n",
    "                output_row.append(cleaned_item)\n",
    "                i=i+1\n",
    "\n",
    "            items_to_str = '\\t'.join(output_row)\n",
    "            f.write(items_to_str + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_files_as_cbioportal():\n",
    "    # clear out the folder, if it has files\n",
    "    dir = dataset_folder + \"/01_with_headers/\" \n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "        print(\"Created Directory : \", dir)\n",
    "    else:\n",
    "        print(\"Directory already existed : \", dir)  \n",
    "        import shutil\n",
    "        for root, dirs, files in os.walk(dir):\n",
    "            for f in files:\n",
    "                os.unlink(os.path.join(root, f))\n",
    "\n",
    "\n",
    "    # loop through all table files\n",
    "    filename = 'patient'\n",
    "    write_file_as_cbioportal(data_clinical_patient, filename)\n",
    "\n",
    "    for df_key in data_clinical_timeline_dfs.keys():\n",
    "        print('Attempt to write '+df_key+'.')\n",
    "        write_file_as_cbioportal( data_clinical_timeline_dfs[df_key], df_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STEP 1: Find patients' first diagnosis date.\n",
      "../Caisis_NonPublicData/Sarcoma_nov2021\n",
      "CHECK NoDxDate? OK\n",
      "TBD: delete existing NoDxDate.txt report.\n",
      "Resulting patient IDs = 4302\n",
      "(4302, 2)\n",
      "\n",
      "STEP 2: Load all CSV tables.\n",
      "Reading Clinical Stages.csv...\n",
      "Clinical Stages read.\n",
      "Reading Demographics.csv...\n",
      "Demographics read.\n",
      "Reading Encounters.csv...\n",
      "Encounters read.\n",
      "Reading Medical Therapy.csv...\n",
      "Medical Therapy read.\n",
      "Reading Pathology.csv...\n",
      "Pathology read.\n",
      "Reading Procedures.csv...\n",
      "Procedures read.\n",
      "Reading Radiation.csv...\n",
      "Radiation read.\n",
      "Reading Social History.csv...\n",
      "Social History read.\n",
      "=====  MUST... import_fields_to_patient_table()\n",
      "\n",
      "STEP 4: Import to events tables.\n",
      "column_names....\n",
      "<class 'list'>\n",
      "['PatientId', 'EVENT_TYPE', 'START_DATE', 'STOP_DATE', 'KPS']\n",
      "---- end of column_names ---\n",
      "datafiles_fields[timeline-encounters]  is....\n",
      "[<__main__.ImportField object at 0x0000020899F106D0>, <__main__.ImportField object at 0x0000020899F103A0>, <__main__.ImportField object at 0x0000020899F108B0>, <__main__.ImportField object at 0x000002089E152970>, <__main__.ImportField object at 0x000002089E152310>]\n",
      "column_names....\n",
      "<class 'list'>\n",
      "['PatientId', 'EVENT_TYPE', 'START_DATE', 'STOP_DATE', 'RadTxType', 'RadTxTarget', 'RadTxTotalDose']\n",
      "---- end of column_names ---\n",
      "datafiles_fields[timeline-radiation]  is....\n",
      "[<__main__.ImportField object at 0x0000020899F106D0>, <__main__.ImportField object at 0x0000020899F103A0>, <__main__.ImportField object at 0x0000020899F108B0>, <__main__.ImportField object at 0x000002089E1524F0>, <__main__.ImportField object at 0x000002089E152BB0>, <__main__.ImportField object at 0x000002089E152940>]\n"
     ]
    }
   ],
   "source": [
    "find_patients_first_dx()\n",
    "print(patients_first_dx.shape)\n",
    "load_all_tables()\n",
    "datafiles_fields = {\n",
    "    'patient': []   #not including PATIENT_ID\n",
    "}\n",
    "print(\"=====  MUST... import_fields_to_patient_table()\")\n",
    "import_fields_to_event_table()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Directory :  ../Caisis_NonPublicData/Sarcoma_nov2021/01_with_headers/\n",
      "end getcoltypes,\n",
      "['STRING', 'DATE']\n",
      "Attempt to write timeline-encounters.\n",
      "in getcoltypes for timeline-encounters,...\n",
      "> STRING\n",
      "> DATE\n",
      "> DATE\n",
      "> NUMBER\n",
      "> STRING\n",
      "end getcoltypes,\n",
      "['STRING', 'STRING', 'DATE', 'DATE', 'NUMBER', 'STRING']\n",
      "Attempt to write timeline-radiation.\n",
      "in getcoltypes for timeline-radiation,...\n",
      "> STRING\n",
      "> DATE\n",
      "> DATE\n",
      "> STRING\n",
      "> STRING\n",
      "> STRING\n",
      "end getcoltypes,\n",
      "['STRING', 'STRING', 'DATE', 'DATE', 'STRING', 'STRING', 'STRING']\n"
     ]
    }
   ],
   "source": [
    "write_files_as_cbioportal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(data_clinical_patient.columns.values))\n",
    "print(data_clinical_patient['BirthDate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zero_dates import square_plus, zero_dates\n",
    "\n",
    "square_plus(3)\n",
    "\n",
    "zero_dates('../Caisis_NonPublicData/Prostate_TAN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_clinical_patient = patients_first_dx.copy()\n",
    "import_fields_to_patient_table()\n",
    "print(data_clinical_patient)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
